{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf0a298f",
   "metadata": {},
   "source": [
    "# Utilizing Agents + RAG to Generate Research Ideas\n",
    "### By: Bradley Sides\n",
    "\n",
    "**Steps:**\n",
    "1. Author inputs their 3(?) most recent papers.\n",
    "2. Related works are located using RAG with sentence embeddings.\n",
    "3. With special consideration placed on the author's own papers, augmented with background context from the related works in addition to the model's own knowledge, generate a \"baseline\" research idea for the author.\n",
    "4. Utilize harsh reviewing agents to assess the idea based on multiple criteria.\n",
    "    \n",
    "    a. Novelty agent checks that the idea is sufficiently different from already researched topics. Provides detailed feedback as well as a score.\n",
    "    \n",
    "    b. Fundability/Impact agent checks that the idea is both competitive for grants and focused on an important topic rather than simply something obscure. Provdes detailed feedback as well as a score.\n",
    "5. If score minimums are not met, return to idea generating agent with feedback from both reviewing agents to improve upon the idea, iterate until passing.\n",
    "6. Once score minimums are met, the idea is finalized.\n",
    "\n",
    "\n",
    "**Models Used:**\n",
    "\n",
    "   LLM: Llama 3 70B 8192\n",
    "\n",
    "   Sentence Encoder: sentence-transformers all-MiniLM-L6-v2\n",
    "\n",
    "   Tokenizer: GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f3a4d",
   "metadata": {},
   "source": [
    "## Load packages, models, environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692e3779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.dataset as ds\n",
    "import glob\n",
    "from transformers import GPT2Tokenizer\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Use your own\n",
    "GROQ_API_KEY = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37bcb5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master DataFrame, pre-cleaned and organized - SEE DATA-CLEANING.IPYNB FOR CLEANED FILE\n",
    "df = pd.read_parquet('compressed_fulldata.parquet')\n",
    "\n",
    "# Sentence Encoder\n",
    "sent_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Language Model: LLama 3\n",
    "    # Num Parameters: 70B\n",
    "    # Context Windoow: 8192\n",
    "GROQ_LLM = ChatGroq(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            groq_api_key=GROQ_API_KEY\n",
    "        )\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6068a90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys and OS settings for Langchain\n",
    "#os.environ['LANGCHAIN TRACING V2'] = 'true'\n",
    "#os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "#os.environ['LANGCHAIN API KEY'] = 'ls__cb6a134591764951b016859dde32b411'\n",
    "#!pip -q install langchain-groq duckduckgo-search\n",
    "#!pip -q install -U langchain_community tiktoken langchainhub\n",
    "#!pip -q install -U langchain langgraph tavily-python\n",
    "#!pip show langgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e4a7bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>pmid</th>\n",
       "      <th>concepts</th>\n",
       "      <th>author</th>\n",
       "      <th>num_citations</th>\n",
       "      <th>related_works</th>\n",
       "      <th>cited_by_api_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Treatment of Alcohol Withdrawal Syndrome</td>\n",
       "      <td>Treatment of the alcohol withdrawal syndrome i...</td>\n",
       "      <td>1994-01-01</td>\n",
       "      <td>7912939</td>\n",
       "      <td>[Alcohol withdrawal syndrome, Medicine, Rehabi...</td>\n",
       "      <td>[Vural Özdemir]</td>\n",
       "      <td>18</td>\n",
       "      <td>[https://openalex.org/W4388336948, https://ope...</td>\n",
       "      <td>https://api.openalex.org/works?filter=cites:W2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title  \\\n",
       "0  Treatment of Alcohol Withdrawal Syndrome   \n",
       "\n",
       "                                            abstract publication_date  \\\n",
       "0  Treatment of the alcohol withdrawal syndrome i...       1994-01-01   \n",
       "\n",
       "      pmid                                           concepts  \\\n",
       "0  7912939  [Alcohol withdrawal syndrome, Medicine, Rehabi...   \n",
       "\n",
       "            author  num_citations  \\\n",
       "0  [Vural Özdemir]             18   \n",
       "\n",
       "                                       related_works  \\\n",
       "0  [https://openalex.org/W4388336948, https://ope...   \n",
       "\n",
       "                                    cited_by_api_url  \n",
       "0  https://api.openalex.org/works?filter=cites:W2...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2e9e4a",
   "metadata": {},
   "source": [
    "## Implement a simple RAG system using sentence embeddings to find the N most similar papers to each paper the author submits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69e21892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50dd9365a5fa418ab30c4234c34c610e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(185275, 384)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TODO: Implement RAG to get \n",
    "    1. Related works (HyDE/Decomposition/RAG Fusion)\n",
    "    2. Top (3?) citations from each paper\n",
    "'''\n",
    "# create sentence embeddings\n",
    "def embed_sentences():\n",
    "    embed = sent_model.encode(df['abstract'].tolist(), show_progress_bar = True)\n",
    "    print(np.array(embed).shape)\n",
    "    np.save('saved_embeddings.npy', embed) # Optional for saving the embeddings to disk\n",
    "    return embed\n",
    "    \n",
    "# find similar papers based on cosine similarity between sentence embeddings\n",
    "def simple_rag(abstract, emb_list, abstract_df, n):\n",
    "    query_emb = sent_model.encode([abstract])[0]\n",
    "    similarities = cosine_similarity([query_emb], emb_list)[0]\n",
    "    # top = np.argsort(similarities)[-n:][::-1] # Indices of top n most similar papers, excluding the paper itself\n",
    "    # USE THIS IF PASSING IN A PAPER FROM DATASET, OTHER IF \n",
    "    top = np.argsort(similarities)[-n-1:][::-1] \n",
    "    top_papers = abstract_df.iloc[top]\n",
    "    return [(row['title'], row['abstract']) for idx, row in top_papers.iterrows()]\n",
    "\n",
    "# print out original and similar papers\n",
    "def print_similar(sim_papers):\n",
    "    \n",
    "    print(\"ORIGINAL PAPER: \")\n",
    "    print(\"________________________________________________\")\n",
    "    print(\"Title: \" + sim_papers[0][0])\n",
    "    print(\"\")\n",
    "    print(\"Abstract: \" + sim_papers[0][1])\n",
    "    print(\"=========================================================\")\n",
    "    for i in range(len(sim_papers)-1):\n",
    "        print(\"++++++++++++++++++++++++++++++++++\")\n",
    "        print(\"Related title Number \" + str((i+1)))\n",
    "        print(\"++++++++++++++++++++++++++++++++++\")\n",
    "        print(\"Title: \" + sim_papers[i+1][0])\n",
    "        print(\"\")\n",
    "        print(\"Abstract: \" + sim_papers[i+1][1])\n",
    "        print(\"=========================================================\")\n",
    "\n",
    "embed = embed_sentences()\n",
    "#abstract = my_abstracts[0]\n",
    "#n = 2 # Number of papers to pull\n",
    "#sim_papers = simple_rag(abstract, embed, df, n) # First entry is the same paper, drop it\n",
    "#print_similar(sim_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ebda55",
   "metadata": {},
   "source": [
    "## Generate the initial idea focused on author's work and utilizing related works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "21ce97c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This will serve as the initial idea generator for the agents to work on\n",
    "'''\n",
    "prompt = PromptTemplate(\n",
    "    template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a research assistant. You are a master at synthesizing information to formulate creative, novel, fundable, and feasible ideas that improve on the previous work that is presented to you.\n",
    "    \n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Conduct a comprehensive analysis of the abstracts and related work provided below and present a well-formulated idea for a new research paper that logically follows the direction of research in my field. \n",
    "    Here are my 3 previous papers, from most to least recent:\n",
    "    My recent papers: \\n\\n {og_papers} \\n\\n\n",
    "    \n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables = [\"og_papers\"]\n",
    ")\n",
    "\n",
    "og_papers = my_data\n",
    "base_idea_gen = prompt | GROQ_LLM | StrOutputParser()\n",
    "#idea = base_idea.invoke({\"og_papers\": og_papers})\n",
    "#print(idea)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2230bb1",
   "metadata": {},
   "source": [
    "## Introduce Reviewing Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025f3754",
   "metadata": {},
   "source": [
    "### 1: Fundability and Impact Reviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4df79038",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "REVIEWING AGENT #1: FUNDABILITY AND IMPACT REVIEWER\n",
    "    \n",
    "** Note: Utilizing prompts from researchAgent paper heavily for this, will need to make new ones \n",
    "'''\n",
    "fundability_agent_prompt = PromptTemplate(template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are an AI assistant whose primary goal is to meticulously evaluate the fundability of research ideas based off of the NSF funding criteria in order to aid researchers in refining their approaches based on your evaluations and feedback, thereby amplifying the quality and impact of their scientific contributions.\n",
    "    \\n\\n\n",
    "    You are going to evaluate a research idea for its potential fundability by the NSF. Refer to the target papers to help understand the context of the problem for a more comprehensive assessment.\n",
    "    \n",
    "    The existing studies are: {og_papers}\n",
    "    \n",
    "    Now, proceed with your evaluation approach that should be systematic:\n",
    "        - Start by thoroughly reading the experiment design and its rationale, keeping in mind the context provided by the research problem, scientific method, and existing studies mentioned above.\n",
    "        - Next, generate a review and feedback that should be constructive, helpful, and concise, focusing on the fundability of the experiment.\n",
    "        - Finally, provide a score on a 5-point Likert scale, with 1 being the lowest, please ensuring a discerning and critical evaluation to avoid a tendency towards uniformly high ratings (4-5) unless fully justified:\n",
    "        \n",
    "    Criteria to consider:\n",
    "        - Quality and potential to advance knowledge: Does the project propose high-quality activities that can transform the frontiers of knowledge?\n",
    "        - Contribution to societal goals: How does the project contribute to broader societal goals?\n",
    "        - Metrics for evaluation: Are the metrics for meaningful assessment and evaluation appropriate and well-defined?\n",
    "        - Originality and Creativity: Are the ideas creative, original, or potentially transformative?\n",
    "        - Plan and Rationale: Is the project plan well-reasoned and organized? Does it include mechanisms to assess success?\n",
    "        - Qualifications: How well qualified are the individuals or teams proposing the project?\n",
    "        - Resources: Are adequate resources available to carry out the activities proposed?\n",
    "    \n",
    "    I am going to provide you with the research idea here: {final_idea}\n",
    "    \n",
    "    After your evaluation of the above content, please provide your review, feedback, and rating, in the format of:\n",
    "    Review: \n",
    "    Feedback:\n",
    "    Rating (1-5):\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables = [\"final_idea\", \"og_papers\"]\n",
    ")\n",
    "\n",
    "fundability = fundability_agent_prompt | GROQ_LLM | StrOutputParser()\n",
    "\n",
    "#print(fundability.invoke({\"my_data\": my_data, \"idea\": idea}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c5d77",
   "metadata": {},
   "source": [
    "### 2: Novelty Reviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "22acc22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "REVIEWING AGENT #2: NOVELY REVIEWER\n",
    "\n",
    "Description: This should pull the top N most similar papers to the \"original\" idea and produce a \"novelty\" score, as well as provide feedback.\n",
    "'''\n",
    "novelty_agent_prompt = PromptTemplate(template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are an AI assistant whose primary goal is to meticulously evaluate the novelty of research ideas based off given criteria in order to aid researchers in refining their approaches based on your evaluations and feedback, thereby amplifying the quality and impact of their scientific contributions.\n",
    "    \\n\\n\n",
    "    The process for evaluating an idea for its novelty is outlined as follows:\n",
    "        - Begin by understanding the essence of the proposed idea, focusing particularly on its unique aspects and claims of novelty.\n",
    "        - Compare the idea against a broad spectrum of existing studies, considering both direct and tangential relevance.\n",
    "    \n",
    "    The criteria for novelty assessment are:\n",
    "        - Innovation: Evaluate if the idea introduces any new methodologies, tools, or conceptual frameworks.\n",
    "        - Transformation Potential: Assess whether the idea has the potential to significantly shift current practices or theoretical understanding.\n",
    "        - Differentiation: Examine how distinct the idea is from existing studies. Highlight specific elements that set it apart.\n",
    "        - Feasibility of New Approaches: Consider the practical implementation of the idea, evaluating if the innovative aspects are achievable within the current technological and resource constraints.\n",
    "        - Impact on Existing Knowledge: Determine the potential impact of the idea on stimulating further research or development in its field.\n",
    "        - Interdisciplinary Merit: Consider if the idea brings together diverse fields or disciplines in a way that fosters new directions or insights.\n",
    "\n",
    "        Given the research idea presented here: {final_idea}\n",
    "        Evaluate it against both your own knowledge of related work, as well as the following papers deemed similar through sentence embedding: {most_similar_papers}\n",
    "        Please proceed with your systematic evaluation, and provide a detailed review that includes:\n",
    "            - Your assessment of how the idea meets each novelty criterion.\n",
    "            - Constructive feedback on areas where the idea could be further differentiated or developed.\n",
    "            - A rating on a 5-point scale regarding its overall novelty, where 1 indicates very little novelty and 5 indicates highly novel.\n",
    "\n",
    "    assistant\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables = [\"final_idea\", \"most_similar_papers\"]\n",
    ")\n",
    "novelty = novelty_agent_prompt | GROQ_LLM | StrOutputParser()\n",
    "#most_similar_papers = simple_rag(idea, embed, df, n=10)\n",
    "#print(novelty.invoke({\"idea\": idea, \"most_similar_papers\": most_similar_papers}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d35d272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: REWRITING AGENTS:\n",
    "    1. Novelty Rewriter\n",
    "    2. Fundability/Impact Rewriter\n",
    "'''  \n",
    "#Description: These should take feedback from reviewing agents, rewrite according to critiques, and then pass back to the reviewers\n",
    "\n",
    "\n",
    "novelty_analysis_prompt = PromptTemplate(\n",
    "    template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are an expert at evaluating the feedback from reviewers on a research idea based on its novelty and deciding if the idea needs to be updated. \\n\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    This is the research idea: {final_idea} \\n\n",
    "    \n",
    "    this feedback was given on the idea: {novelty_feedback} \\n\n",
    "    \n",
    "    If the score is lower than 4.5, return EXACTLY: rewrite\n",
    "    If the score is 4.5 or higher, return EXACTLY: no_rewrite\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\"\"\",\n",
    "    input_variables= [\"final_idea\", \"novelty_feedback\"]\n",
    ")\n",
    "\n",
    "fundability_analysis_prompt = PromptTemplate(\n",
    "    template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are an expert at evaluating the feedback from reviewers on a research idea based on its fundability and potential and deciding if the idea needs to be updated. \\n\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    This is the research idea: {final_idea} \\n\n",
    "    \n",
    "    this feedback was given on the idea: {fundability_feedback} \\n\n",
    "    \n",
    "    If the score is lower than 4.5, return EXACTLY: rewrite\n",
    "    If the score is 4.5 or higher, return EXACTLY: no_rewrite\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\"\"\",\n",
    "    input_variables= [\"final_idea\", \"fundability_feedback\"]\n",
    ")\n",
    "\n",
    "nov2 = novelty_analysis_prompt | GROQ_LLM | StrOutputParser()\n",
    "fund2 = fundability_analysis_prompt | GROQ_LLM | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a7ec3073",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_idea_prompt = PromptTemplate(\n",
    "    template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are the final agent in charge of producing novel, fundable, impactful, and feasible research ideas. A research idea has been formulated and reviewed and you are tasked with incorporating the feedback into the idea and potentially augmenting it based on what is recommended.\n",
    "    You must not sacrifice one criteria for the improvement of another, so be careful with your augmentation and utilize the feedback as best you can.\n",
    "    You should produce either an updated version of the idea given to you based on the feedback, or the same idea given to you if augmentation is not necessary. As a baseline, both scores should be above 4.5 Provide justification for any change you make.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    The research idea is: {final_idea}\n",
    "    \n",
    "    \n",
    "    The feedback from the fundability and impact reviewer is: {fundability_feedback}\n",
    "\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables = [\"final_idea\", \"fundability_feedback\"]\n",
    ")\n",
    "\n",
    "rewrite = rewrite_idea_prompt | GROQ_LLM | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c740a5",
   "metadata": {},
   "source": [
    "## Build the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c0359b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langgraph.graph import END, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3cbe2a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the graph\n",
    "    \n",
    "    Attributes: \n",
    "        og_papers: abstract and title of first X papers from main author\n",
    "        sim_papers: abstract and title of Y similar papers to each of og_papers\n",
    "        most_similar_papers: abstract and title of Z similar papers to the IDEA\n",
    "        idea: LLM generated idea\n",
    "        updated_idea: LLM generated idea\n",
    "        novelty_feedback: LLM generated critiques on novelty\n",
    "        fundability_feedback: LLM generated critiques on fundability/impact\n",
    "        num_steps: number of steps taken\n",
    "    \"\"\"\n",
    "    og_papers : str\n",
    "    sim_papers : List[List[str]]\n",
    "    most_similar_papers : List[List[str]]\n",
    "    base_idea : str\n",
    "    final_idea : str\n",
    "    #novelty_feedback : str\n",
    "    fundability_feedback : str\n",
    "    #novelty_analysis: str\n",
    "    fundability_analysis: str\n",
    "    num_steps : int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9e2a0d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_docs(state):\n",
    "    '''Get related papers'''\n",
    "    print(\"---FINDING RELATED PAPERS----\")\n",
    "    og_papers = state['og_papers']\n",
    "    sim_papers = state['sim_papers']\n",
    "    num_steps = int(state['num_steps'])\n",
    "    num_steps += 1\n",
    "    for paper in og_papers:\n",
    "        p = simple_rag(paper, embed, df, n=2)\n",
    "        sim_papers.append(p)\n",
    "    print_similar(sim_papers)\n",
    "    return{\"sim_papers\": sim_papers, \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0b459635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Edit this function AND base_idea to include sim_docs\n",
    "def init_idea(state):\n",
    "    '''Generate initial idea'''\n",
    "    print(\"---GENERATING INITIAL IDEA---\")\n",
    "    og_papers = state['og_papers']\n",
    "    base_idea = state['base_idea']\n",
    "    num_steps = int(state['num_steps'])\n",
    "    num_steps += 1\n",
    "    base_idea = base_idea_gen.invoke({\"og_papers\": og_papers})\n",
    "    final_idea = base_idea # for updating\n",
    "    print(base_idea)\n",
    "    #write_markdown_file(base_idea, \"base_idea\")\n",
    "    return {\"base_idea\": base_idea, \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b862a65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_fundability(state):\n",
    "    '''Assess fundability and impact of the idea'''\n",
    "    print(\"---ASSESSING FUNDABILITY & IMPACT OF IDEA---\")\n",
    "    og_papers = state['og_papers']\n",
    "    final_idea = state['final_idea']\n",
    "    fundability_feedback = state['fundability_feedback']\n",
    "    num_steps = int(state['num_steps'])\n",
    "    num_steps += 1\n",
    "    \n",
    "    fundability_feedback = fundability.invoke({\"final_idea\": final_idea, \"og_papers\": og_papers})\n",
    "    print(fundability_feedback)\n",
    "    #write_markdown_file(fundability_feedback, \"fundability_feedback\")\n",
    "    return {\"fundability_feedback\": fundability_feedback, \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "dcb31846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_novelty(state):\n",
    "    '''Assess novelty of idea'''\n",
    "    print(\"---ASSESSING NOVELTY OF IDEA---\")\n",
    "    og_papers = state['og_papers']\n",
    "    final_idea = state['final_idea']\n",
    "    most_similar_papers = state['most_similar_papers']\n",
    "    novelty_feedback = state['novelty_feedback']\n",
    "    num_steps = int(state['num_steps'])\n",
    "    num_steps += 1\n",
    "    \n",
    "    most_similar_papers = simple_rag(final_idea, embed, df, n=8)\n",
    "    novelty_feedback = novelty.invoke({\"final_idea\": final_idea, \"most_similar_papers\": most_similar_papers})\n",
    "    print(novelty_feedback)\n",
    "    #write_markdown_file(novelty_feedback, \"novelty_feedback\")\n",
    "    return {\"novelty_feedback\": novelty_feedback, \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e0a3b8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_novelty(state):\n",
    "    '''Decide on update based novelty of idea'''\n",
    "    print(\"---DECIDING ON UPDATE VIA: NOVELTY---\")\n",
    "    novelty_feedback = state['novelty_feedback']\n",
    "    novelty_analysis = state['novelty_analysis']\n",
    "    num_steps = int(state['num_steps'])\n",
    "    num_steps += 1\n",
    "    \n",
    "    novelty_analysis = nov2.invoke({\"final_idea\": final_idea, \"novelty_feedback\": novelty_feedback})\n",
    "    print(novelty_analysis)\n",
    "    #write_markdown_file(novelty_feedback, \"novelty_feedback\")\n",
    "    return {\"novelty_analysis\": novelty_analysis, \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d314685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_fundability(state):\n",
    "    '''Decide on update based fundability of idea'''\n",
    "    print(\"---DECIDING ON UPDATE VIA: FUNDABILITY & IMPACT---\")\n",
    "    final_idea = state['final_idea']\n",
    "    fundability_feedback = state['fundability_feedback']\n",
    "    fundability_analysis = state['fundability_analysis']\n",
    "    \n",
    "    fundability_analysis = fund2.invoke({\"final_idea\": final_idea, \"fundability_feedback\": fundability_feedback})\n",
    "    print(fundability_analysis)\n",
    "    #write_markdown_file(novelty_feedback, \"novelty_feedback\")\n",
    "    updated_state = state.copy()\n",
    "    updated_state['fundability_analysis'] = fundability_analysis\n",
    "    updated_state['num_steps'] = int(state['num_steps']) + 1\n",
    "    return updated_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ad9c4ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_idea(state):\n",
    "    '''Rewrite the idea based on feedback'''\n",
    "    print(\"---REWRITING IDEA---\")\n",
    "    \n",
    "    final_idea = state['final_idea']\n",
    "    #novelty_feedback = state['novelty_feedback']\n",
    "    fundability_feedback = state['fundability_feedback']\n",
    "    num_steps = int(state['num_steps'])\n",
    "    num_steps += 1\n",
    "    \n",
    "    final_idea = rewrite.invoke({\"final_idea\": final_idea, \"fundability_feedback\": fundability_feedback})\n",
    "    print(final_idea)\n",
    "    return {\"final_idea\": final_idea, \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c06ed7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_more(state):\n",
    "    print(\"NO MORE UPDATES NEEDED\")\n",
    "    final_idea = state['final_idea']\n",
    "    base_idea = state['base_idea']\n",
    "    num_steps = int(state['num_steps'])\n",
    "    num_steps += 1\n",
    "    \n",
    "    #write_markdown_file(base_idea, \"base_idea\")\n",
    "    return {\"final_idea\": final_idea, \"num_steps\": num_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "abbd478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_printer(state):\n",
    "    \"\"\"Print the state\"\"\"\n",
    "    print(\"---STATE PRINTER---\")\n",
    "    print(f\"base_idea Idea: {state['base_idea']}\\n\")\n",
    "    print(f\"Final Idea: : {state['final_idea']}\\n\")\n",
    "    print(f\"Similar (to my own) Papers: {state['sim_papers']}\\n\")\n",
    "    print(f\"Related (to idea) Papers: {state['most_similar_papers']}\\n\")\n",
    "    print(f\"Fundability Feedback: : {state['fundability_feedback']}\\n\")\n",
    "    #print(f\"Novelty Feedback: {state['novelty_feedback']}\\n\")\n",
    "    print(f\"Num Steps: {state['num_steps']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209291be",
   "metadata": {},
   "source": [
    "### Conditional Edge(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b4bc723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional Edge\n",
    "def route_to_rewrite(state):\n",
    "    print(\"---ROUTE TO REWRITE---\")\n",
    "    final_idea = state['final_idea']\n",
    "    #novelty_feedback = state['novelty_feedback']\n",
    "    fundability_feedback = state['fundability_feedback']\n",
    "    #novelty_analysis = state['novelty_analysis']\n",
    "    fundability_analysis = state['fundability_analysis']\n",
    "    print(fundability_analysis)\n",
    "    if fundability_analysis == \"rewrite\":\n",
    "        print(\"---ROUTE TO REWRITE---\")\n",
    "        return \"rewrite\"\n",
    "    else:\n",
    "        print(\"---ROUTE TO FINAL---\")\n",
    "        return \"no_rewrite\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df905a",
   "metadata": {},
   "source": [
    "## Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ce135ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Nodes:\n",
    "#workflow.add_node(\"find_similar_docs\", find_similar_docs)\n",
    "workflow.add_node(\"init_idea\", init_idea)\n",
    "workflow.add_node(\"assess_fundability\", assess_fundability)\n",
    "#workflow.add_node(\"assess_novelty\", assess_novelty)\n",
    "workflow.add_node(\"analyze_fundability\", analyze_fundability)\n",
    "#workflow.add_node(\"analyze_novelty\", analyze_novelty)\n",
    "workflow.add_node(\"rewrite_idea\", rewrite_idea)\n",
    "workflow.add_node(\"no_more\", no_more)\n",
    "workflow.add_node(\"state_printer\", state_printer)\n",
    "\n",
    "# Edges: \n",
    "workflow.set_entry_point(\"init_idea\")\n",
    "#workflow.add_edge(\"find_similar_docs\", \"init_idea\")\n",
    "workflow.add_edge(\"init_idea\", \"assess_fundability\")\n",
    "workflow.add_edge(\"assess_fundability\", \"analyze_fundability\")\n",
    "#workflow.add_edge(\"assess_novelty\", \"analyze_novelty\")\n",
    "#workflow.add_conditional_edges(\n",
    "    #\"analyze_novelty\", \n",
    "     #route_to_rewrite,\n",
    "    #{\n",
    "         #\"rewrite\": \"rewrite_idea\",\n",
    "         #\"no_rewrite\": \"analyze_fundability\"\n",
    "     #}      \n",
    "#)\n",
    "workflow.add_conditional_edges(\n",
    "    \"analyze_fundability\",\n",
    "    route_to_rewrite,\n",
    "    {\n",
    "        \"rewrite\": \"rewrite_idea\",\n",
    "        \"no_rewrite\": \"no_more\"\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"rewrite_idea\", \"assess_fundability\")\n",
    "workflow.add_edge(\"no_more\", \"state_printer\")\n",
    "workflow.add_edge(\"state_printer\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "de828e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf27d7d5",
   "metadata": {},
   "source": [
    "## Sandbox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "62b34384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in original papers: 1625\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SANDBOX AREA!!!\n",
    "Practice with authors\n",
    "\"\"\"\n",
    "\n",
    "# Get most recent 5 papers from specific author\n",
    "filter_df = df[df['author'] =='Wei Chen']\n",
    "sorted_df = filter_df.sort_values(by='publication_date', ascending = False)\n",
    "recent_papers = sorted_df.head(5)\n",
    "recent_papers\n",
    "\n",
    "# Format for passing into prompt template\n",
    "my_abstracts = []\n",
    "for i in range(4):\n",
    "    concat = \"TITLE: \" + recent_papers.iloc[i]['title'] + \"\\nABSTRACT: \" + recent_papers.iloc[i]['abstract']\n",
    "    my_abstracts.append(concat)\n",
    "my_data = '\\n\\n'.join(my_abstracts)    \n",
    "token_count = len(tokenizer.tokenize(my_data))\n",
    "print(\"Number of tokens in original papers: \" + str(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada11295",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = {\"og_papers\": my_data, \"num_steps\":0}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Finished running: {key}:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6e10599b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING INITIAL IDEA---\n",
      "After conducting a comprehensive analysis of the abstracts and related work provided, I have formulated a well-formulated idea for a new research paper that logically follows the direction of research in your field.\n",
      "\n",
      "**Research Paper Idea:**\n",
      "\n",
      "**Title:** \"Evaluating the Impact of AI-generated Content on the Integrity of Scientific Publishing: A Large-scale Analysis of Peer Review and Manuscript Quality\"\n",
      "\n",
      "**Background:** The rapid advancement of AI technology has made text generation tools increasingly accessible, scalable, and effective. While these tools have the potential to revolutionize scientific communication, they also pose a significant threat to the credibility of scientific literature if used for plagiarism or manipulation.\n",
      "\n",
      "**Research Question:** How can we detect and mitigate the potential misuse of AI-generated content in scientific publishing, and what are the implications for peer review and manuscript quality?\n",
      "\n",
      "**Methodology:**\n",
      "\n",
      "1. **Data Collection:** Utilize a large dataset of manuscripts from open-access publications, including those with AI-generated content.\n",
      "2. **AI-generated Content Detection:** Develop and apply machine learning algorithms to detect AI-generated content in manuscripts, building upon existing paraphrase identification approaches.\n",
      "3. **Peer Review Analysis:** Investigate the impact of AI-generated content on peer review outcomes, including review scores, acceptance rates, and subjective review quality.\n",
      "4. **Manuscript Quality Assessment:** Evaluate the quality of manuscripts with AI-generated content, focusing on accessibility, readability, and explainability features.\n",
      "\n",
      "**Expected Outcomes:**\n",
      "\n",
      "1. **Detection of AI-generated Content:** Development of a reliable method for detecting AI-generated content in scientific manuscripts.\n",
      "2. **Impact on Peer Review:** Insights into how AI-generated content affects peer review outcomes, including potential biases and inconsistencies.\n",
      "3. **Manuscript Quality:** Assessment of the quality of manuscripts with AI-generated content, highlighting potential issues with accessibility, readability, and explainability.\n",
      "\n",
      "**Implications:**\n",
      "\n",
      "1. **Scientific Integrity:** This study will provide valuable insights into the potential risks and consequences of AI-generated content in scientific publishing.\n",
      "2. **Peer Review Practices:** The findings will inform the development of guidelines and best practices for detecting and addressing AI-generated content in peer review.\n",
      "3. **Manuscript Quality Standards:** The study will contribute to the development of standards for manuscript quality, ensuring that scientific communication remains accurate, reliable, and accessible.\n",
      "\n",
      "**Novelty and Contribution:**\n",
      "\n",
      "This study combines the themes of AI-generated content, peer review, and manuscript quality, building upon the previous research in papers 1-4. By investigating the impact of AI-generated content on scientific publishing, this study addresses a critical gap in the existing literature and provides a crucial contribution to the ongoing discussion on the integrity of scientific communication.\n",
      "Finished running: init_idea:\n",
      "---ASSESSING FUNDABILITY & IMPACT OF IDEA---\n",
      "Since there is no research idea provided, I will evaluate the fundability of each paper individually based on the NSF funding criteria.\n",
      "\n",
      "**PAPER 1:**\n",
      "Review: This study investigates the association between author-suggested reviewers and review invitation, review scores, acceptance rates, and subjective review quality. The large dataset of 8K manuscripts from 46K authors and 21K reviewers provides a robust foundation for the analysis.\n",
      "\n",
      "Feedback: To enhance the fundability of this project, the authors could provide more context on the significance of their findings, particularly in terms of how they can improve the peer-review process. Additionally, the authors could elaborate on the implications of their results for the broader scientific community.\n",
      "\n",
      "Rating: 3\n",
      "\n",
      "**PAPER 2:**\n",
      "Review: This study develops computational techniques to measure the accessibility, readability, and explainability of figures in scientific communication. The high accuracy achieved in detecting problems is promising, and the release of the analysis as a dataset and methods for further examination is a significant contribution.\n",
      "\n",
      "Feedback: To strengthen the fundability of this project, the authors could provide more insights into the broader implications of their findings for scientific communication and the potential impact on the dissemination of scientific knowledge. Additionally, the authors could explore potential applications of their methodology in other domains.\n",
      "\n",
      "Rating: 4\n",
      "\n",
      "**PAPER 3:**\n",
      "Review: This study explores the factors that differentiate and predict the longevity of shared resources in articles, such as URLs to code or data. The analysis of an extensive repository of publications and the reconstruction of how they looked at different time points provide valuable insights.\n",
      "\n",
      "Feedback: To enhance the fundability of this project, the authors could provide more context on the broader implications of their findings for scientific reproducibility and the preservation of scientific datasets. Additionally, the authors could elaborate on potential strategies for disseminating and creating standards for resource sharing.\n",
      "\n",
      "Rating: 4\n",
      "\n",
      "**PAPER 4:**\n",
      "Review: This study reviews traditional and current approaches to paraphrase identification and proposes a refined typology of paraphrases. The investigation into how under-representation of certain types of paraphrases impacts detection capabilities is a valuable contribution.\n",
      "\n",
      "Feedback: To strengthen the fundability of this project, the authors could provide more insights into the potential applications of their typology and the broader implications of their findings for detecting plagiarism in scientific literature. Additionally, the authors could explore potential collaborations with stakeholders in the scientific publishing industry.\n",
      "\n",
      "Rating: 4\n",
      "Finished running: assess_fundability:\n",
      "---DECIDING ON UPDATE VIA: FUNDABILITY & IMPACT---\n",
      "Based on the feedback, I would recommend the following:\n",
      "\n",
      "**PAPER 1:** Rewrite\n",
      "**PAPER 2:** no_rewrite\n",
      "**PAPER 3:** no_rewrite\n",
      "**PAPER 4:** no_rewrite\n",
      "---ROUTE TO REWRITE---\n",
      "Based on the feedback, I would recommend the following:\n",
      "\n",
      "**PAPER 1:** Rewrite\n",
      "**PAPER 2:** no_rewrite\n",
      "**PAPER 3:** no_rewrite\n",
      "**PAPER 4:** no_rewrite\n",
      "---ROUTE TO FINAL---\n",
      "Finished running: analyze_fundability:\n",
      "NO MORE UPDATES NEEDED\n",
      "Finished running: no_more:\n",
      "---STATE PRINTER---\n",
      "base_idea Idea: After conducting a comprehensive analysis of the abstracts and related work provided, I have formulated a well-formulated idea for a new research paper that logically follows the direction of research in your field.\n",
      "\n",
      "**Research Paper Idea:**\n",
      "\n",
      "**Title:** \"Evaluating the Impact of AI-generated Content on the Integrity of Scientific Publishing: A Large-scale Analysis of Peer Review and Manuscript Quality\"\n",
      "\n",
      "**Background:** The rapid advancement of AI technology has made text generation tools increasingly accessible, scalable, and effective. While these tools have the potential to revolutionize scientific communication, they also pose a significant threat to the credibility of scientific literature if used for plagiarism or manipulation.\n",
      "\n",
      "**Research Question:** How can we detect and mitigate the potential misuse of AI-generated content in scientific publishing, and what are the implications for peer review and manuscript quality?\n",
      "\n",
      "**Methodology:**\n",
      "\n",
      "1. **Data Collection:** Utilize a large dataset of manuscripts from open-access publications, including those with AI-generated content.\n",
      "2. **AI-generated Content Detection:** Develop and apply machine learning algorithms to detect AI-generated content in manuscripts, building upon existing paraphrase identification approaches.\n",
      "3. **Peer Review Analysis:** Investigate the impact of AI-generated content on peer review outcomes, including review scores, acceptance rates, and subjective review quality.\n",
      "4. **Manuscript Quality Assessment:** Evaluate the quality of manuscripts with AI-generated content, focusing on accessibility, readability, and explainability features.\n",
      "\n",
      "**Expected Outcomes:**\n",
      "\n",
      "1. **Detection of AI-generated Content:** Development of a reliable method for detecting AI-generated content in scientific manuscripts.\n",
      "2. **Impact on Peer Review:** Insights into how AI-generated content affects peer review outcomes, including potential biases and inconsistencies.\n",
      "3. **Manuscript Quality:** Assessment of the quality of manuscripts with AI-generated content, highlighting potential issues with accessibility, readability, and explainability.\n",
      "\n",
      "**Implications:**\n",
      "\n",
      "1. **Scientific Integrity:** This study will provide valuable insights into the potential risks and consequences of AI-generated content in scientific publishing.\n",
      "2. **Peer Review Practices:** The findings will inform the development of guidelines and best practices for detecting and addressing AI-generated content in peer review.\n",
      "3. **Manuscript Quality Standards:** The study will contribute to the development of standards for manuscript quality, ensuring that scientific communication remains accurate, reliable, and accessible.\n",
      "\n",
      "**Novelty and Contribution:**\n",
      "\n",
      "This study combines the themes of AI-generated content, peer review, and manuscript quality, building upon the previous research in papers 1-4. By investigating the impact of AI-generated content on scientific publishing, this study addresses a critical gap in the existing literature and provides a crucial contribution to the ongoing discussion on the integrity of scientific communication.\n",
      "\n",
      "Final Idea: : None\n",
      "\n",
      "Similar (to my own) Papers: None\n",
      "\n",
      "Related (to idea) Papers: None\n",
      "\n",
      "Fundability Feedback: : Since there is no research idea provided, I will evaluate the fundability of each paper individually based on the NSF funding criteria.\n",
      "\n",
      "**PAPER 1:**\n",
      "Review: This study investigates the association between author-suggested reviewers and review invitation, review scores, acceptance rates, and subjective review quality. The large dataset of 8K manuscripts from 46K authors and 21K reviewers provides a robust foundation for the analysis.\n",
      "\n",
      "Feedback: To enhance the fundability of this project, the authors could provide more context on the significance of their findings, particularly in terms of how they can improve the peer-review process. Additionally, the authors could elaborate on the implications of their results for the broader scientific community.\n",
      "\n",
      "Rating: 3\n",
      "\n",
      "**PAPER 2:**\n",
      "Review: This study develops computational techniques to measure the accessibility, readability, and explainability of figures in scientific communication. The high accuracy achieved in detecting problems is promising, and the release of the analysis as a dataset and methods for further examination is a significant contribution.\n",
      "\n",
      "Feedback: To strengthen the fundability of this project, the authors could provide more insights into the broader implications of their findings for scientific communication and the potential impact on the dissemination of scientific knowledge. Additionally, the authors could explore potential applications of their methodology in other domains.\n",
      "\n",
      "Rating: 4\n",
      "\n",
      "**PAPER 3:**\n",
      "Review: This study explores the factors that differentiate and predict the longevity of shared resources in articles, such as URLs to code or data. The analysis of an extensive repository of publications and the reconstruction of how they looked at different time points provide valuable insights.\n",
      "\n",
      "Feedback: To enhance the fundability of this project, the authors could provide more context on the broader implications of their findings for scientific reproducibility and the preservation of scientific datasets. Additionally, the authors could elaborate on potential strategies for disseminating and creating standards for resource sharing.\n",
      "\n",
      "Rating: 4\n",
      "\n",
      "**PAPER 4:**\n",
      "Review: This study reviews traditional and current approaches to paraphrase identification and proposes a refined typology of paraphrases. The investigation into how under-representation of certain types of paraphrases impacts detection capabilities is a valuable contribution.\n",
      "\n",
      "Feedback: To strengthen the fundability of this project, the authors could provide more insights into the potential applications of their typology and the broader implications of their findings for detecting plagiarism in scientific literature. Additionally, the authors could explore potential collaborations with stakeholders in the scientific publishing industry.\n",
      "\n",
      "Rating: 4\n",
      "\n",
      "Num Steps: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Let's do it with Acuna papers:\n",
    "abstract_1 = \"### PAPER 1: Peer review is an important part of science, aimed at providing expert and objective assess- ment of a manuscript. Because of many factors, including time constraints, unique expertise needs, and deference, many journals ask authors to suggest peer reviewers for their own manuscript. Previous researchers have found differing effects about this practice that might be inconclusive due to sample sizes. In this article, we analyze the association between author-suggested reviewers and review invitation, review scores, acceptance rates, and subjective review quality using a large dataset of close to 8K manuscripts from 46K authors and 21K reviewers from the journal PLOS ONE’s Neuroscience section. We found that all- author-suggested review panels increase the chances of acceptance by 20 percent points vs all-editor-suggested panels while agreeing to review less often. While PLOS ONE has since ended the practice of asking for suggested reviewers, many others still use them and perhaps should consider the results presented here.\"\n",
    "abstract_2 = \"### PAPER 2: Figures are an essential part of scientific communication. Yet little is understood about how accessible (e.g., color-blind safe), readable (e.g., good contrast), and explainable (e.g., contain captions and legends) they are. We develop computational techniques to measure these features and analyze a large sample of them from open access publications. Our method combines computer and human vision research principles, achieving high accuracy in detecting problems. In our sample, we estimated that around 20.6% of publications contain either accessibility, readability, or explainability issues (around 2% of all figures contain accessibility issues, 3% of diagnostic figures contain readability issues, and 23% of line charts contain explainability issues). We release our analysis as a dataset and methods for further examination by the scientific community.\"\n",
    "abstract_3 = \"### PAPER 3: Research has shown that most resources shared in articles (e.g., URLs to code or data) are not kept up to date and mostly disappear from the web after some years (Zeng et al., 2019). Little is known about the factors that differentiate and predict the longevity of these resources. This article explores a range of explanatory features related to the publication venue, authors, references, and where the resource is shared. We analyze an extensive repository of publications and, through web archival services, reconstruct how they looked at different time points. We discover that the most important factors are related to where and how the resource is shared, and surprisingly little is explained by the author’s reputation or prestige of the journal. By examining the places where long-lasting resources are shared, we suggest that it is critical to disseminate and create standards with modern technologies. Finally, we discuss implications for reproducibility and recognizing scientific datasets as first-class citizens.\"\n",
    "abstract_4 = \"### PAPER 4: The rapid advancement of AI technology has made text generation tools like GPT-3 and ChatGPT increasingly accessible, scalable, and effective. This can pose serious threat to the credibility of various forms of media if these technologies are used for plagiarism, including scientific literature and news sources. Despite the development of automated methods for paraphrase identification, detecting this type of plagiarism remains a challenge due to the disparate nature of the datasets on which these methods are trained. In this study, we review traditional and current approaches to paraphrase identification and propose a refined typology of paraphrases. We also investigate how this typology is represented in popular datasets and how under-representation of certain types of paraphrases impacts detection capabilities. Finally, we outline new directions for future research and datasets in the pursuit of more effective paraphrase detection using AI.\"\n",
    "abstracts = [abstract_1, abstract_2, abstract_3, abstract_4]\n",
    "concat = ' '.join(abstracts)\n",
    "\n",
    "inputs = {\"og_papers\": concat, \"num_steps\":0}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"Finished running: {key}:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ddc4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
